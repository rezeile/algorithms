<html>

<head>
<title>Lecture 20: Amortized Analysis</title>
<link rel="stylesheet" type="text/css" href="../../styles/3110.css">
<script src="../../styles/colorize.js" type="text/javascript"></script>
</head>

<body class=notes>
<h1>Lecture 20: Amortized Analysis</h1>

<p>The claim that hash tables have <i>O</i>(1) expected performance
for lookup and insert is based on the assumption that the number of
elements stored in the table is comparable to the number of buckets.
If a hash table has many more elements than buckets, the number of
elements stored at each bucket will become large.  For instance, with
a constant number of buckets and <i>O</i>(<i>n</i>) elements, the lookup time is
<i>O</i>(<i>n</i>) and not <i>O</i>(1).</p>

<p>The solution to this problem is to increase the size of the table
when the number of elements in the table gets too large compared to the
size of the table.  If we let the <def>load factor</def> be the ratio
of the number of elements to the number of buckets, when the load factor
exceeds some small constant (typically 2 for chaining and .75 for
linear probing), we allocate a new table, typically double the size
of the old table, and rehash all the elements into the new table.
This operation is not constant time, but rather
linear in the number of elements at the time the table is
grown.</p>

<p>The linear running time of a resizing operation is not as much of a
problem as it might sound (although it can be an issue for some
real-time computing systems).  If the table is doubled in size every time
it is needed, then the resizing operation occurs with exponentially
decreasing frequency.  As a consequence, the insertion
of <i><span class=m>n</span></i> elements into an empty array
takes only <span class=m><i>O</i>(<i>n</i>)</span> time in all, including
the cost of resizing. We say that the insertion operation
has <span class=m><i>O</i>(1)</span>
<def>amortized run time</def> because the time required to insert an
element is <span class=m><i>O</i>(1)</span> <em>on average</em>, even though
some elements trigger a lengthy rehashing of all the elements of the
hash table.</p>

<p>It is crucial that the array size grow geometrically
(doubling). It might be tempting to grow the array by a fixed
increment (e.g., 100 elements at time), but this results in asymptotic
linear rather than constant amortized running time.</p>

<p>Now we turn to a more detailed description of amortized analysis.</p>

<h2>Amortized Analysis</h2>

<p>Amortized analysis is a worst-case analysis of a
a <em>sequence</em> of operations &mdash; to obtain a tighter bound on the
overall or average cost per operation in the sequence than is obtained
by separately analyzing each operation in the sequence.  For instance,
when we considered the union and find operations for the disjoint set
data abstraction earlier in the semester, we were able to bound the
running time of individual operations by <i>O</i>(log <i>n</i>). However, for
a sequence of <i>n</i> operations, it is possible to obtain tighter
than an <i>O</i>(<i>n</i> log <i>n</i>) bound (although that analysis is more
appropriate to 4820 than to this course).  Here we will consider a
simplified version of the hash table problem above, and show that a
sequence of <i>n</i> insert operations has overall time <i>O</i>(<i>n</i>).  

<p>There are three main techniques used for amortized analysis:
<ul>
<li>The <def>aggregate method</def>, where the total running time for a sequence
of operations is analyzed.
<li>The <def>accounting</def> (or <def>banker's</def>) method, where we impose
an extra charge on inexpensive operations and use it to pay for expensive operations
later on.
<li>The <def>potential</def> (or <def>physicist's</def>) method, in which we
derive a <em>potential function</em> characterizing the amount of extra work we can
do in each step.  This potential either increases or
decreases with each successive operation, but cannot be negative.
</ul>

<p>Consider an extensible array that can store an arbitrary number of integers,
like an <code>ArrayList</code> or <code>Vector</code> in Java.  These are
implemented in terms of ordinary (non-extensible) arrays.  Each <code>add</code>
operation inserts a new element after all the elements previously inserted.
If there are no empty cells left, a new array of double the size is allocated,
and all the data from the old array is copied to the corresponding entries
in the new array.  For instance, consider the following sequence of insertions,
starting with an array of length 1:
<pre>
           +--+
Insert 11  |11|
           +--+
           +--+--+
Insert 12  |11|12|
           +--+--+
           +--+--+--+--+
Insert 13  |11|12|13|  |
           +--+--+--+--+
           +--+--+--+--+
Insert 14  |11|12|13|14|
           +--+--+--+--+
           +--+--+--+--+--+--+--+--+
Insert 15  |11|12|13|14|15|  |  |  |
           +--+--+--+--+--+--+--+--+
</pre>

<p class=cont>The table is doubled in the second, third, and fifth steps.
As each insertion takes <i>O</i>(<i>n</i>) time in the worst case, a simple
analysis would yield a bound of <i>O</i>(<i>n</i><sup>2</sup>) time for <i>n</i>
insertions.  But it is not this bad.  Let's analyze a sequence of <i>n</i>
operations using the three methods.

<h3>Aggregate Method</h3>

<p>Let <i>c<sub>i</sub></i> be the cost of the <i>i</i>-th insertion:
<pre>
<i>c<sub>i</sub></i> = <i>i</i> if <i>i&minus;1</i> is a power of 2
     1 otherwise
</pre>
<p>Let's consider the size of the table <i>s<sub>i</sub></i> and the
cost <i>c<sub>i</sub></i> for the first few insertions in a sequence:
<pre>
<i>i</i>   1  2  3  4  5  6  7  8  9 10
<i>s<sub>i</sub></i>  1  2  4  4  8  8  8  8 16 16
<i>c<sub>i</sub></i>  1  2  3  1  5  1  1  1  9  1
</pre>

<p>Alteratively we can see
that <i>c<sub>i</sub></i>=1+<i>d<sub>i</sub></i> where
<i>d<sub>i</sub></i> is the cost of doubling the table size. That is
<pre>
<i>d<sub>i</sub></i> = <i>i&minus;1</i> if <i>i&minus;1</i> is a power of 2
     0 otherwise
</pre>
Then summing over the entire sequence, all the 1's sum to <i>O</i>(<i>n</i>),
and all the <i>d<sub>i</sub></i> also sum to <i>O</i>(<i>n</i>).  That is,
<pre>
&Sigma;<i><sub>1&le;i&le;n</sub> c<sub>i</sub></i>   &le;   n + &Sigma;<i><sub>0&le;j&le;m</sub> 2<sup>j&minus;1</sup></i> 
</pre>
<p class=cont>where <i>m</i> = log(<i>n</i> &minus; 1)</i>.  Both terms on the right hand side of the
inequality are <i>O</i>(<i>n</i>), so the total running time of <i>n</i>
insertions is <i>O</i>(<i>n</i>).

<h3>Accounting (Banker's) Method</h3>

<p>The aggregate method directly seeks a bound on the overall running time of a
sequence of operations.  In contrast, the accounting method seeks to find a <em>payment</em>
of a number of extra time units charged to each individual operation such that the sum of
the payments is an upper bound on the total actual cost.  Intuitively, one can think of
maintaining a bank account.  Low-cost operations are charged a little bit
more than their true cost, and the surplus is deposited into
the bank account for later use.  High-cost operations can then be charged less than their
true cost, and the deficit is paid for by the savings in the bank account.
In that way we spread the cost of high-cost operations
over the entire sequence.  The charges to each operation must be set large
enough that the balance in the bank account always remains positive, but small enough
that no one operation is charged significantly more than its actual cost.

<p>We emphasize that the extra time charged to an operation does not
mean that the operation really takes that much time.  It is just a method of accounting
that makes the analysis easier.

<p>If we let <i>c'<sub>i</sub></i> be the charge for the <i>i</i>-th
operation and <i>c<sub>i</sub></i> be the true cost, then we would like
<p>&Sigma;<i><sub>1&le;i&le;n</sub> c<sub>i</sub></i> &le; 
&Sigma;<i><sub>1&le;i&le;n</sub> c'<sub>i</sub></i>
<p class=cont>for all <i>n</i>, which says that the <def>amortized time</def> &Sigma;<i><sub>1&le;i&le;n</sub> c'<sub>i</sub></i> for that sequence of <i>n</i> operations is a bound on the true time &Sigma;<i><sub>1&le;i&le;n</sub> c<sub>i</sub></i>.

<p>Back to the example of the extensible array.  Say it costs 1 unit to insert
an element and 1 unit to move it when the table is doubled.
Clearly a charge of 1 unit per insertion is not enough, because there
is nothing left over to pay for the moving.  A charge of 2 units per
insertion again is not enough, but a charge of 3 appears to be:
<pre>
<i>i</i>   1  2  3  4  5  6  7  8  9 10
<i>s<sub>i</sub></i>  1  2  4  4  8  8  8  8 16 16
<i>c<sub>i</sub></i>  1  2  3  1  5  1  1  1  9  1
<i>c'<sub>i</sub></i> 3  3  3  3  3  3  3  3  3  3
<i>b<sub>i</sub></i>  2  3  3  5  3  5  7  9  3  4
</pre>
<p class=cont>where <i>b<sub>i</sub></i> is the balance after the <i>i</i>-th
insertion.

<p>In fact, this is enough in general.  Let <i>m</i> refer to the <i>m</i>-th element inserted.
The three units charged to <i>m</i> are spent as follows:
<ul>
<li>One unit is used to insert <i>m</i> immediately into the table.</li>
<li>One unit is used to move <i>m</i> the first time the table is grown after <i>m</i> is inserted.</li>
<li>One unit is donated to element <i>m &minus; 2<sup>k</sup></i>, where <i>2<sup>k</sup></i> is the largest power of 2 not exceeding <i>m</i>, and is used to move that element the first time the table is grown after <i>m</i> is inserted.</li>
</ul>
Now whenever an element is moved, the move is already paid for.  The first time an element is moved,
it is paid for by one of its own time units that was charged to it when it was inserted; and all 
subsequent moves are paid for by donations from elements inserted later.

<p>In fact, we can do slightly better, by charging just 1 for the first
insertion and then 3 for each insertion after that, because for the
first insertion there are no elements to copy.  This will yield a zero
balance after the first insertion and then a positive one thereafter.

<h3>Potential (Physicist's) Method</h3>

<p>Above we saw the aggregate method and the banker's method for dealing with extensible arrays.
Now let us look at the physicist's method.</p>

<p>Suppose we can define a <def>potential function</def> <span class=m>&Phi;</span> (read "Phi")
on states of a data structure with the following properties:</p>
<ul>
<li><span class=m>&Phi;(<i>h</i><sub>0</sub>) = 0</span>, where <span class=m><i>h</i><sub>0</sub></span> is the initial state of the data structure.</li>
<li><span class=m>&Phi;(<i>h</i><sub><i>t</i></sub>) &ge; 0</span> for all states <span class=m><i>h</i><sub><i>t</i></sub></span> of the data structure occurring during the course of the computation.</li>
</ul>
<p class=cont>Intuitively, the potential function will keep track of the precharged time at
any point in the computation. It measures how much saved-up time is available to pay for expensive operations.
It is analogous to the bank balance in the banker's method.  But interestingly, it depends only on the
current state of the data structure, irrespective of the history of the computation
that got it into that state.</p>  

<p>We then define the <def>amortized time</def> of an operation as</p>

<blockquote class=m>
<p>
<span class=m><i>c</i> + &Phi;(<i>h</i>') &minus; &Phi;(<i>h</i>)</span>,
</p>
</blockquote>

<p class=cont>where <span class=m><i>c</i></span> is the actual cost of the operation and
<span class=m><i>h</i></span> and <span class=m><i>h</i>'</span> are the states of the data
structure before and after the operation, respectively.  Thus the amortized time is the
actual time plus the change in potential.  Ideally, <span class=m>&Phi;</span> should
be defined so that the amortized time of each operation is small.  Thus the change in potential
should be positive for low-cost operations and negative for high-cost operations.</p>

<p>Now consider a sequence of <i class=m>n</i> operations
taking actual times <span class=m><i>c</i><sub>0</sub>, <i>c</i><sub>1</sub>,
<i>c</i><sub>2</sub>, ..., <i>c</i><sub><i>n</i>&minus;1</sub></span>
and producing data structures
<span class=m><i>h</i><sub>1</sub>, <i>h</i><sub>2</sub>, ..., <i>h</i><sub><i>n</i></sub></span>
starting from <span class=m><i>h</i><sub>0</sub></span>.
The total amortized time is the sum of the individual amortized times:</p>

<blockquote class=m>
(<span class=m><i>c</i><sub>0</sub> + &Phi;(<i>h</i><sub>1</sub>) &minus; &Phi;(<i>h</i><sub>0</sub>))
+ (<i>c</i><sub>1</sub> + &Phi;(<i>h</i><sub>2</sub>) &minus; &Phi;(<i>h</i><sub>1</sub>)) + ... + (<i>c</i><sub><i>n</i>&minus;1</sub> + &Phi;(<i>h</i><sub><i>n</i></sub>)
&minus; &Phi;(<i>h</i><sub><i>n</i>&minus;1</sub>))<br>
= <i>c</i><sub>0</sub> + <i>c</i><sub>1</sub> + ... + <i>c</i><sub><i>n</i>&minus;1</sub> + &Phi;(<i>h</i><sub><i>n</i></sub>) &minus; &Phi;(<i>h</i><sub>0</sub>)</span><br>
= <i>c</i><sub>0</sub> + <i>c</i><sub>1</sub> + ... + <i>c</i><sub><i>n</i>&minus;1</sub> + &Phi;(<i>h</i><sub><i>n</i></sub>)</span>.
</blockquote>

<p class=cont>Therefore the amortized time for a sequence of operations overestimates of
the actual time by <span class="m">&Phi;(<i>h</i><sub><i>n</i></sub>)</span>,
which by assumption is always positive.  Thus
the total amortized time is always an upper bound on the actual time.
</p>

<p>For dynamically resizable arrays with resizing by doubling, we can use the potential function</p>

<blockquote class=m>
<p>
<span class=m>&Phi;(<i>h</i>) = 2<i>n</i> &minus; <i>m</i></span>,
</p>
</blockquote>
<p class=cont>where <span class=m><i>n</i></span> is the current number of elements and <span class=m><i>m</i></span> is the current length of the array.  If we start with an array of length 0 and allocate an array of length 1 when the first element is added, and thereafter double the array size whenever we need more space, we have <span class=m>&Phi;(<i>h</i><sub>0</sub>) = 0</span> and <span class=m>&Phi;(<i>h</i><sub><i>t</i></sub>) &ge; 0</span> for all <span class=m><i>t</i></span>.  The latter inequality holds because the number of elements is always at least half the size of the array.</p>

<p>Now we would like to show that adding an element takes amortized constant time.  There are two cases.</p>
<ul>
<li>If <span class=m><i>n</i> &lt; <i>m</i></span>, then the actual cost is 1, <span class=m><i>n</i></span> increases by 1, and <span class=m><i>m</i></span> does not change.  Then the potential increases by 2,
so the amortized time is 1 + 2 = 3.</li>
<li>If <span class=m><i>n</i> = <i>m</i></span>, then the array is doubled, so the actual time is <span class=m><i>n</i> + 1</span>.
But the potential drops from <i class=m>n</i> to <span class=m>2</span>,
so amortized time is <span class=m><i>n</i> + 1 + (2 &minus; <i>n</i>) = 3</span>.</li>
</ul>
<p class=cont>In both cases, the amortized time is O(1).</p>

<p>The key to amortized analysis with the physicist's method is to define the right potential function.
The potential function needs to save up enough time to be used later when it
is needed. But it cannot save so much time that it causes the amortized time of
the current operation to be too high.
</p>

<script type="text/javascript">colorize_all()</script>
</body>
</html>
